# Generated by Django 6.0.2 on 2026-02-11 19:39

from django.db import migrations


def deduplicate_addresses(apps, schema_editor):
    """Merge Address rows that differ only by case using bulk SQL.

    For each group of case-insensitive duplicates we keep the oldest row
    (lowest pk), re-point every Location FK to it, and delete the rest.
    Original casing is preserved on the kept row.
    """
    execute = schema_editor.execute

    # 1. Re-point Location FKs from duplicate Addresses to the keeper.
    execute(
        """
        UPDATE common_location SET address_id = keeper.keep_id
        FROM (
            SELECT MIN(id) AS keep_id,
                   COALESCE(LOWER(street), '') AS norm_street,
                   COALESCE(LOWER(city), '') AS norm_city,
                   COALESCE(LOWER(state), '') AS norm_state,
                   COALESCE(LOWER(zip_code), '') AS norm_zip
            FROM common_address
            GROUP BY norm_street, norm_city, norm_state, norm_zip
            HAVING COUNT(*) > 1
        ) keeper
        JOIN common_address dup
          ON COALESCE(LOWER(dup.street), '') = keeper.norm_street
         AND COALESCE(LOWER(dup.city), '') = keeper.norm_city
         AND COALESCE(LOWER(dup.state), '') = keeper.norm_state
         AND COALESCE(LOWER(dup.zip_code), '') = keeper.norm_zip
         AND dup.id != keeper.keep_id
        WHERE common_location.address_id = dup.id
        """
    )

    # 2. Delete the duplicate Address rows.
    execute(
        """
        DELETE FROM common_address
        WHERE id IN (
            SELECT dup.id
            FROM common_address dup
            JOIN (
                SELECT MIN(id) AS keep_id,
                       COALESCE(LOWER(street), '') AS norm_street,
                       COALESCE(LOWER(city), '') AS norm_city,
                       COALESCE(LOWER(state), '') AS norm_state,
                       COALESCE(LOWER(zip_code), '') AS norm_zip
                FROM common_address
                GROUP BY norm_street, norm_city, norm_state, norm_zip
                HAVING COUNT(*) > 1
            ) keeper
              ON COALESCE(LOWER(dup.street), '') = keeper.norm_street
             AND COALESCE(LOWER(dup.city), '') = keeper.norm_city
             AND COALESCE(LOWER(dup.state), '') = keeper.norm_state
             AND COALESCE(LOWER(dup.zip_code), '') = keeper.norm_zip
             AND dup.id != keeper.keep_id
        )
        """
    )

    # 3. Round all Location GPS points to 5 decimal places (~1.1 m precision)
    #    to eliminate sub-metre jitter duplicates.
    GPS_PRECISION = 5
    execute(
        """
        UPDATE common_location
        SET point = ST_SetSRID(
            ST_MakePoint(
                ROUND(ST_X(point::geometry)::numeric, %s),
                ROUND(ST_Y(point::geometry)::numeric, %s)
            ),
            ST_SRID(point::geometry)
        )
        WHERE ROUND(ST_X(point::geometry)::numeric, %s) != ST_X(point::geometry)
           OR ROUND(ST_Y(point::geometry)::numeric, %s) != ST_Y(point::geometry)
        """,
        [GPS_PRECISION, GPS_PRECISION, GPS_PRECISION, GPS_PRECISION],
    )

    # 4. After address dedup + GPS rounding, Locations may now be duplicated
    #    (same address + point + poi). Deduplicate those too.
    deduplicate_locations(apps, schema_editor)


def deduplicate_locations(apps, schema_editor):
    """Merge Location rows that share the same address + point + point_of_interest.

    Keeps the oldest (lowest pk), re-points Note/HmisNote FKs, deletes the rest.
    """
    execute = schema_editor.execute

    # Discover FK tables that reference Location.
    fk_tables = []
    for label in ("notes.Note", "hmis.HmisNote"):
        try:
            model = apps.get_model(label)
            fk_tables.append(model._meta.db_table)
        except LookupError:
            pass

    # Re-point each FK table.
    for table in fk_tables:
        execute(
            f"""
            UPDATE {table} SET location_id = keeper.keep_id
            FROM (
                SELECT MIN(id) AS keep_id, address_id, point, point_of_interest
                FROM common_location
                GROUP BY address_id, point, point_of_interest
                HAVING COUNT(*) > 1
            ) keeper
            JOIN common_location dup
              ON dup.address_id IS NOT DISTINCT FROM keeper.address_id
             AND dup.point = keeper.point
             AND dup.point_of_interest IS NOT DISTINCT FROM keeper.point_of_interest
             AND dup.id != keeper.keep_id
            WHERE {table}.location_id = dup.id
            """
        )

    # Delete duplicate Locations.
    execute(
        """
        DELETE FROM common_location
        WHERE id IN (
            SELECT dup.id
            FROM common_location dup
            JOIN (
                SELECT MIN(id) AS keep_id, address_id, point, point_of_interest
                FROM common_location
                GROUP BY address_id, point, point_of_interest
                HAVING COUNT(*) > 1
            ) keeper
              ON dup.address_id IS NOT DISTINCT FROM keeper.address_id
             AND dup.point = keeper.point
             AND dup.point_of_interest IS NOT DISTINCT FROM keeper.point_of_interest
             AND dup.id != keeper.keep_id
        )
        """
    )


class Migration(migrations.Migration):

    dependencies = [
        ("common", "0018_fix_file_extensions"),
    ]

    operations = [
        migrations.RunPython(
            deduplicate_addresses,
            reverse_code=migrations.RunPython.noop,
        ),
    ]
